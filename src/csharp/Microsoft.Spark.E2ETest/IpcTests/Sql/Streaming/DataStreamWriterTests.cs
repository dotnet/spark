// Licensed to the .NET Foundation under one or more agreements.
// The .NET Foundation licenses this file to you under the MIT license.
// See the LICENSE file in the project root for more information.

using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using Microsoft.Spark.E2ETest.Utils;
using Microsoft.Spark.Sql;
using Microsoft.Spark.Sql.Streaming;
using Microsoft.Spark.Sql.Types;
using Xunit;

namespace Microsoft.Spark.E2ETest.IpcTests
{
    [Collection("Spark E2E Tests")]
    public class DataStreamWriterTests
    {
        private readonly SparkSession _spark;

        public DataStreamWriterTests(SparkFixture fixture)
        {
            _spark = fixture.Spark;
        }

        /// <summary>
        /// Test signatures for APIs up to Spark 2.3.*.
        /// </summary>
        [Fact]
        public void TestSignaturesV2_3_X()
        {
            DataFrame df = _spark
                .ReadStream()
                .Format("rate")
                .Option("rowsPerSecond", 1)
                .Load();

            DataStreamWriter dsw = df.WriteStream();

            Assert.IsType<DataStreamWriter>(dsw.OutputMode("append"));

            Assert.IsType<DataStreamWriter>(dsw.OutputMode(OutputMode.Append));

            Assert.IsType<DataStreamWriter>(dsw.Format("json"));

            Assert.IsType<DataStreamWriter>(dsw.Option("stringOption", "value"));
            Assert.IsType<DataStreamWriter>(dsw.Option("boolOption", true));
            Assert.IsType<DataStreamWriter>(dsw.Option("longOption", 1L));
            Assert.IsType<DataStreamWriter>(dsw.Option("doubleOption", 3D));

            Assert.IsType<DataStreamWriter>(
                dsw.Options(
                    new Dictionary<string, string>
                    {
                        { "option1", "value1" },
                        { "option2", "value2" }
                    }));

            Assert.IsType<DataStreamWriter>(dsw.PartitionBy("age"));
            Assert.IsType<DataStreamWriter>(dsw.PartitionBy("age", "name"));

            Assert.IsType<DataStreamWriter>(dsw.QueryName("queryName"));

            Assert.IsType<DataStreamWriter>(dsw.Trigger(Trigger.Once()));
        }

        [SkipIfSparkVersionIsLessThan(Versions.V2_4_0)]
        public void TestForeach()
        {
            // Temporary folder to put our test stream input.
            using var srcTempDirectory = new TemporaryDirectory();
            string streamInputPath = Path.Combine(srcTempDirectory.Path, "streamInput");

            // [1, 2, ..., 99]
            _spark.Range(1, 100).Write().Json(streamInputPath);

            // Temporary folder the TestForeachWriter will write to.
            using var dstTempDirectory = new TemporaryDirectory();
            int partitions = 3;

            // Read streamInputPath, repartitions data into 3, then
            // calls TestForeachWriter on the data.
            DataStreamWriter dsw = _spark
                .ReadStream()
                .Schema("id INT")
                .Json(streamInputPath)
                .Repartition(partitions)
                .WriteStream()
                .Foreach(new TestForeachWriter(dstTempDirectory.Path));

            //// Trigger the stream batch once.
            dsw.Trigger(Trigger.Once()).Start().AwaitTermination();

            // Verify that TestForeachWriter wrote a unique .csv for each
            // partition.
            Assert.Equal(
                partitions,
                Directory.GetFiles(dstTempDirectory.Path, "*.csv").Length);

            // Read in the *.csv file(s) generated by the TestForeachWriter.
            // If there are multiple input files, sorting by "id" will make
            // validation simpler.
            DataFrame foreachWriterOutputDF = _spark
                .Read()
                .Schema("id INT")
                .Csv(dstTempDirectory.Path)
                .Sort("id");

            // [101, 102, ..., 199]
            IEnumerable<int> expected = Enumerable.Range(101, 99);

            Assert.Equal(
                expected.Select(i => new object[] { i }),
                foreachWriterOutputDF.Collect().Select(r => r.Values));
        }

        [Serializable]
        private class TestForeachWriter : IForeachWriter
        {
            [ThreadStatic]
            private static StreamWriter s_streamWriter;

            private readonly string _writePath;

            public TestForeachWriter(string writePath)
            {
                _writePath = writePath;
            }

            public void Close(Exception errorOrNull)
            {
                s_streamWriter?.Dispose();
            }

            public bool Open(long partitionId, long epochId)
            {
                try
                {
                    s_streamWriter = new StreamWriter(
                        Path.Combine(_writePath,$"sink-foreachWriter-{Guid.NewGuid()}.csv"));
                    return true;
                }
                catch
                {
                    return false;
                }
            }

            public void Process(Row value)
            {
                s_streamWriter.WriteLine(string.Join(",", value.Values.Select(v => 100 + (int)v)));
            }
        }
    }
}
